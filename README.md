# Introducci칩n a las Redes Generativas Adversarias :robot:  + Autocolorizador 游깯
[![HitCount](http://hits.dwyl.io/adriacabeza/GAN-workshop.svg)](http://hits.dwyl.io/adriacabeza/GAN-workshop)[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/adriacabeza/GAN-workshop)[![GitHub stars](https://img.shields.io/github/stars/adriacabeza/Unnamed.svg)](https://GitHub.com/adriacabeza/GAN-workshop/stargazers/)[![GitHub repo size in bytes](https://img.shields.io/github/repo-size/adriacabeza/GAN-workshop.svg)](https://github.com/adriacabeza/GAN-workshop)[![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adriacabeza/GAN-workshop/)


## Charla: Introducci칩n a las Redes Generativas Adversarias
El contenido m치s te칩rico de la charla est치 disponible en las [slides](slides/slides.pdf).

## Proyecto: Autocolorizador

Adem치s para ver un uso de las GANs y su implementaci칩n, se ha hecho este peque침o proyecto que pretende colorizar im치genes en blanco y negro usando una conditional GAN. 

### Prerequisites
- Para instalar los requerimientos, usa el requirements.txt (vas a necesitarlos para preparar el dataset).
```bash
pip install -r requirements.txt
```

### Resultados
Aqu칤 podemos ver algunos resultados obtenidos (hand-picked) tras un d칤a de training en un dataset de 36.500 im치genes: 

| Results |
|--------|
|   ![](docs/output11.png)   |
|   ![](docs/output1.png)    |
|   ![](docs/output2.png)    |
|   ![](docs/output3.png)    |
|   ![](docs/output5.png)    |
|   ![](docs/output6.png)    |
|   ![](docs/output4.png)    |
|   ![](docs/output7.png)    |
|   ![](docs/output8.png)    |
|   ![](docs/output9.png)    |
|   ![](docs/output.png)   |

Adem치s puedes descargarte los weights del generador por si qu칠reis hacer pruebas: **[generador](./weights/generator.h5)**.


### Arquitectura
Este c칩digo est치 basado en la arquitectura **pix2pix** (por *Isola et al*). Pese a disponer de un generador y un discriminador con objetivos opuestos en busca de un equilibrio de Nash, esta arquitectura es diferente a las GANs normales pues est치 no coge como input ruido aleatorio (espacio latente),  en esta en cambio cogemos como input toda una imagen *x*. Nuestro objetivo es **traducirla** en otra imagen con una estructura similar. Es decir, nuestro generador *G* tiene que producir *G(X)* el cual tendr치 que ser indistinguible de *y* (la otra imagen con una estructura similar) para nuestro discriminador *D*.


Sus partes principales son:

- **Generador U-NET**: el generador de la pix2pix se parece mucho a un **autoencoder**. Coge la imagen que tiene que ser traducida, la comprime a un espacio de menos dimensiones llamado **Cuello de Botella** y luego aprende a hacer upsampling para conseguir la imagen deseada como output. 

  Adem치s tambi칠n tiene ciertos parecidos con una ResNet en la manera en como la informaci칩n de capas previas es introducida a las siguientes usando las llamadas **skip connections**. En esta arquitectura disponemos de skip connections que salen de la mitad encoder de la red y van a la otra mitad decoder. Esto nos sirve para prevenir que perdamos informaci칩n en el cuello de botella.

<p align="center">
  <img src="docs/U-net.png">
</p>

- **Discriminador Patch-GAN**: en este discriminador en vez de coger las im치genes y clasificarlas en verdaderas o falsas, se clasifican individualmente diferentes trozos de la imagen as칤 se refuerza el objetivo de conseguir detalles mucho m치s n칤tidos. Adem치s es m치s r치pido de clasificar toda una imagen ya que solo tiene que clasificar peque침os trozos y eso significa menos par치metros.

<p align="center">
  <img src="docs/patch_gan.png">
</p>



### Dataset
Como dataset se ha usado un subset de [Places365](http://places2.csail.mit.edu/download.html) de 1GB aproximadamente (podr칤a haber usado uno m치s grande pero la combinaci칩: internet de pueblo + confinamiento, no es muy buena).

Para preparar el dataset para el notebook, necesitamos que cada par de im치genes (blanco y negro y colorida) esten combinadas en la misma im치gen (mitad y mitad). Para hacerlo primero hay que descargarse las im치genes a color y luego usar los siguientes scripts disponibles en la carpeta dataset:
```
python3 create_grayscale.py --original ../data/original --grayscale ../data/gray_scale
python3 combine_images.py --original ../data/original --grayscale ../data/gray_scale
```

Esto os crear치 una carpeta con el dataset preparado.







